---
title: Qwen2 VL Fine-Tuning
description: Qwen2 VL ëª¨ë¸ FIne-Tuning
head:
  - - meta
    - name: keywords
      content: ê³µë¶€ê¸°ë¡, í•™ìŠµì¼ê¸°, ì§€ì‹ì €ì¥ì†Œ, ê³µë¶€ë¡œê·¸, í•™ìŠµê´€ë¦¬, ê³µë¶€ë…¸íŠ¸, ì§€ì‹ì •ë¦¬, ê³µë¶€ë°©ë²•, í•™ìŠµì €ì¥, ê¸°ì–µë³´ì¡°
  - - meta
    - property: og:title
      content: ğŸ“š ë‘ë‡Œ ì €ì¥ì†Œ - ì¬ë¯¸ìˆëŠ” ê³µë¶€ ê¸°ë¡ ë†€ì´í„°
  - - meta
    - property: og:description
      content: "ê¹Œë¨¹ì§€ ë§ê³  ì¬ë°Œê²Œ ì €ì¥í•˜ì!" ğŸ¯ ê³µë¶€í•œ ë‚´ìš©ì„ ììœ ë¡­ê²Œ ê¸°ë¡í•˜ëŠ” ì¦ê±°ìš´ ì§€ì‹ ì €ì¥ì†Œ
  - - meta
    - property: og:image
      content: https://doc.empasy.com/images/favicon.png
  - - meta
    - property: og:url
      content: https://doc.empasy.com/study/
sort: 400
---

# Qwen2-VL íŒŒì¸íŠœë‹ ê°€ì´ë“œ by LLM

## ğŸ¯ íŒŒì¸íŠœë‹ ì „ ì¤€ë¹„ì‚¬í•­

### 1. **í™˜ê²½ ì„¤ì •**

```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install transformers>=4.37.0
pip install accelerate
pip install datasets
pip install peft
pip install torch>=2.0.0
pip install torchvision
pip install pillow
pip install einops
pip install timm

# Qwen2-VL ì „ìš© íŒ¨í‚¤ì§€
pip install git+https://github.com/QwenLM/Qwen2-VL.git
```

### 2. **í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­**

- **GPU ë©”ëª¨ë¦¬**:
  - Full Fine-tuning: 80GB+ (A100 80GB)
  - LoRA Fine-tuning: 24GB+ (RTX 4090)
  - QLoRA: 16GB+ (V100 16GB)

## ğŸ“Š ë°ì´í„°ì…‹ í˜•ì‹

### 1. **í‘œì¤€ ë°ì´í„° í˜•ì‹**

```json
{
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"
    },
    {
      "from": "gpt",
      "value": "ì´ ì´ë¯¸ì§€ì—ëŠ” ë¹¨ê°„ìƒ‰ ìŠ¤í¬ì¸ ì¹´ê°€ ë„ë¡œ ìœ„ì— ì£¼ì°¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    }
  ],
  "image": "base64_encoded_image_or_image_path"
}
```

### 2. **ë‹¤ì¤‘ í„´ ëŒ€í™” í˜•ì‹**

```json
{
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nì´ ì‚¬ì§„ì˜ ë°°ê²½ì€ ì–´ë””ì¸ê°€ìš”?"
    },
    {
      "from": "gpt",
      "value": "ë„ì‹œì˜ ê±°ë¦¬ì…ë‹ˆë‹¤."
    },
    {
      "from": "human",
      "value": "ì°¨ëŸ‰ì˜ ìƒ‰ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?"
    },
    {
      "from": "gpt",
      "value": "ë¹¨ê°„ìƒ‰ì…ë‹ˆë‹¤."
    }
  ],
  "image": "base64_encoded_image"
}
```

## ğŸ”§ íŒŒì¸íŠœë‹ ë°©ë²•

### 1. **Full Fine-tuning**

```python
# full_finetune.py
import torch
from transformers import (
    Qwen2VLForConditionalGeneration,
    Qwen2VLProcessor,
    TrainingArguments,
    Trainer
)
from datasets import Dataset
import json

# ëª¨ë¸ ë¡œë“œ
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

processor = Qwen2VLProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

# ë°ì´í„°ì…‹ ì¤€ë¹„
def load_dataset(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    def process_example(example):
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        image = Image.open(example['image_path']).convert('RGB')

        # ëŒ€í™” ì²˜ë¦¬
        conversations = example['conversations']
        text = processor.apply_chat_template(conversations, add_generation_prompt=False)

        return {
            'pixel_values': processor.image_processor(image, return_tensors="pt").pixel_values[0],
            'input_ids': processor.tokenizer(text, return_tensors="pt").input_ids[0],
            'labels': processor.tokenizer(text, return_tensors="pt").input_ids[0]
        }

    return Dataset.from_list(data).map(process_example, batched=False)

train_dataset = load_dataset("train_data.json")

# Training Arguments
training_args = TrainingArguments(
    output_dir="./qwen2-vl-finetuned",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    num_train_epochs=3,
    fp16=True,
    logging_steps=10,
    save_steps=500,
    eval_steps=500,
    warmup_steps=100,
    weight_decay=0.01,
    gradient_checkpointing=True,
    remove_unused_columns=False,
)

# Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=lambda data: {
        'pixel_values': torch.stack([d['pixel_values'] for d in data]),
        'input_ids': torch.nn.utils.rnn.pad_sequence(
            [d['input_ids'] for d in data],
            batch_first=True,
            padding_value=processor.tokenizer.pad_token_id
        ),
        'labels': torch.nn.utils.rnn.pad_sequence(
            [d['labels'] for d in data],
            batch_first=True,
            padding_value=-100
        ),
    }
)

# í•™ìŠµ ì‹œì‘
trainer.train()
trainer.save_model()
```

### 2. **LoRA íŒŒì¸íŠœë‹**

```python
# lora_finetune.py
from peft import LoraConfig, get_peft_model
from transformers import Qwen2VLForConditionalGeneration

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# ëª¨ë¸ì— LoRA ì ìš©
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# í•™ìŠµë¥ ì„ ë” ë†’ê²Œ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./qwen2-vl-lora",
    per_device_train_batch_size=4,  # LoRAëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ
    gradient_accumulation_steps=2,
    learning_rate=1e-4,  # ë” ë†’ì€ í•™ìŠµë¥ 
    num_train_epochs=5,
    fp16=True,
    # ... ë‚˜ë¨¸ì§€ ì„¤ì •ì€ ë™ì¼
)
```

### 3. **QLoRA íŒŒì¸íŠœë‹ (4bit ì–‘ìí™”)**

```python
# qlora_finetune.py
from transformers import BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

# 4bit ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto"
)

# QLoRA ì¤€ë¹„
model = prepare_model_for_kbit_training(model)

# LoRA ì„¤ì • (QLoRA)
lora_config = LoraConfig(
    r=8,  # ë” ì‘ì€ rank
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # ì£¼ìš” ëª¨ë“ˆë§Œ ëŒ€ìƒ
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
```

## ğŸ“ ë°ì´í„° ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°

```python
# data_utils.py
from PIL import Image
import base64
import io

def image_to_base64(image_path):
    """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def base64_to_image(base64_str):
    """base64ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    image_data = base64.b64decode(base64_str)
    return Image.open(io.BytesIO(image_data))

def create_conversation_dataset(image_paths, questions, answers):
    """ë°ì´í„°ì…‹ ìƒì„± ë„ìš°ë¯¸"""
    dataset = []
    for img_path, question, answer in zip(image_paths, questions, answers):
        dataset.append({
            "conversations": [
                {
                    "from": "human",
                    "value": f"<image>\n{question}"
                },
                {
                    "from": "gpt",
                    "value": answer
                }
            ],
            "image": image_to_base64(img_path)
        })
    return dataset

def save_dataset(dataset, output_path):
    """ë°ì´í„°ì…‹ ì €ì¥"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
```

## ğŸš€ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì œ

```bash
#!/bin/bash
# train_qwen2_vl.sh

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MODEL_NAME="Qwen/Qwen2-VL-7B-Instruct"
export DATA_PATH="train_data.json"
export OUTPUT_DIR="./qwen2-vl-finetuned"
export BATCH_SIZE=2
export LEARNING_RATE=2e-5
export EPOCHS=3

# í•™ìŠµ ì‹¤í–‰
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --master_port=29500 \
    train_qwen2_vl.py \
    --model_name $MODEL_NAME \
    --data_path $DATA_PATH \
    --output_dir $OUTPUT_DIR \
    --per_device_train_batch_size $BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --num_train_epochs $EPOCHS \
    --fp16 \
    --gradient_checkpointing \
    --logging_steps 10 \
    --save_steps 500
```

## ğŸ” í‰ê°€ ë° ì¶”ë¡ 

```python
# evaluate.py
from transformers import pipeline
from PIL import Image

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ
model_path = "./qwen2-vl-finetuned"
vl_pipeline = pipeline(
    "visual-question-answering",
    model=model_path,
    device="cuda:0"
)

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
image = Image.open("test_image.jpg")

# ì¶”ë¡ 
question = "ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"
result = vl_pipeline(image=image, question=question)

print(f"Question: {question}")
print(f"Answer: {result['answer']}")
print(f"Confidence: {result['score']:.4f}")
```

## âš¡ ìµœì í™” íŒ

### 1. **ë©”ëª¨ë¦¬ ìµœì í™”**

```python
# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
model.gradient_checkpointing_enable()

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •
training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,  # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: 8
)

# mixed precision training
training_args = TrainingArguments(
    fp16=True,  # ë˜ëŠ” bf16=True
)
```

### 2. **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**

```python
from transformers import get_cosine_schedule_with_warmup

# ì›œì—… ìŠ¤ì¼€ì¤„ëŸ¬
num_training_steps = len(train_dataloader) * num_epochs
num_warmup_steps = int(0.1 * num_training_steps)

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)
```

## ğŸ¯ íŠ¹ì • íƒœìŠ¤í¬ íŒŒì¸íŠœë‹ ì˜ˆì œ

### 1. **ì´ë¯¸ì§€ ìº¡ì…”ë‹**

```python
def prepare_captioning_data(image_dir, captions_file):
    """ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë°ì´í„° ì¤€ë¹„"""
    dataset = []
    with open(captions_file, 'r', encoding='utf-8') as f:
        for line in f:
            img_name, caption = line.strip().split('\t')
            dataset.append({
                "conversations": [
                    {
                        "from": "human",
                        "value": "<image>\nì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                    },
                    {
                        "from": "gpt",
                        "value": caption
                    }
                ],
                "image": image_to_base64(f"{image_dir}/{img_name}")
            })
    return dataset
```

### 2. **VQA (Visual Question Answering)**

```python
def prepare_vqa_data(questions_file, annotations_file, image_dir):
    """VQA ë°ì´í„° ì¤€ë¹„"""
    # questions ë° annotations íŒŒì¼ ì²˜ë¦¬
    # ...
    return vqa_dataset
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
# wandb ì—°ë™
import wandb

wandb.init(project="qwen2-vl-finetuning")

training_args = TrainingArguments(
    output_dir="./output",
    report_to="wandb",  # wandbì— ë¦¬í¬íŠ¸
    logging_dir="./logs",
    logging_steps=10,
)
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### **Common Issues:**

1. **CUDA Out of Memory**: ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°, ê·¸ë˜ë””ì–¸íŠ¸ accumulation ì‚¬ìš©
2. **NaN Loss**: í•™ìŠµë¥  ë‚®ì¶”ê¸°, gradient clipping ì ìš©
3. **Slow Training**: mixed precision ì‚¬ìš©, ë°ì´í„° ë¡œë”© ìµœì í™”

```python
# ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
training_args = TrainingArguments(
    max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
)
```
